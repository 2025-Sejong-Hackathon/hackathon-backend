# -*- coding: utf-8 -*-
"""[2025-2 해커톤]유저 매칭 모델_백 연결.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ocyKB987icnEV41npuMdnLYtz7lGw70Q
"""

# pip install fastapi uvicorn pandas scikit-learn numpy

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
import json
from typing import List, Dict, Any

W_P = 4  # Personality (성격/예민)
W_S = 3  # Sleep (수면)
W_C = 2  # Cleanliness (청결)

weight_A = {
    "sleep_habit": W_S, "wake_up": W_S, "activity_time": W_S, "out_return": W_S,
    "clean_immediate": W_C, "desk_status": W_C, "clean_cycle": W_C, "other_seat_tol": W_C,
    "phone_noise": W_P, "light_sensitivity": W_P, "key_mouse_noise": W_P, "alarm_habit": W_P,
    "social_willingness": W_P, "friend_invite": W_P, "dorm_stay": W_P, "space_privacy": W_P
}

class DormMatchAI_Server:
    def __init__(self, data_path):
        self.data_path = data_path
        self.users_df = None
        self.weighted_features_df = None # DF 형태로 저장
        self.weights = weight_A
        self.scaler = MinMaxScaler() # 스케일러를 인스턴스 변수로 저장 (중요)
        self.kmeans = KMeans(n_clusters=12, random_state=42) # 모델 저장

        # 텍스트 매핑
        self.text_map = {
            # [생활 패턴]
            "sleep_habit": {0: "12시전 취침", 1: "새벽 취침"},
            "wake_up": {0: "늦잠/오후 기상", 1: "아침 기상"},
            "activity_time": {0: "낮 활동(아침형)", 1: "밤 활동(올빼미)"},
            "out_return": {0: "상관없음", 1: "연락/알림 필요"}, # 1이 예민(Sensitive)
            "dorm_stay": {0: "주로 밖에서 보냄", 1: "주로 기숙사에 있음"}, # 1이 내향(Introvert) -> 방콕

            # [청결도]
            "clean_cycle": {0: "매일 청소", 1: "3일마다", 2: "1주마다", 3: "1달마다"},
            "clean_immediate": {0: "나중에 치움", 1: "바로바로 치움"}, # 0이 Dirty, 1이 Clean
            "desk_status": {0: "어수선함(인간미)", 1: "깔끔하게 정리"}, # 0이 Dirty, 1이 Clean
            "other_seat_tol": {0: "상관없음", 1: "내 자리 건들지마"}, # 1이 예민

            # [소음/예민함]
            "phone_noise": {0: "안에서 통화 OK", 1: "밖에서 통화"}, # 1이 예민/매너
            "light_sensitivity": {0: "불 켜도 잘 잠", 1: "불 꺼야 잠"}, # 1이 예민
            "key_mouse_noise": {0: "상관없음", 1: "무소음 선호"}, # 1이 예민
            "alarm_habit": {0: "잘 못 듣는 편", 1: "바로 끄고 일어남"}, # 1이 예민/즉각반응
            "space_privacy": {0: "물건 공유 가능", 1: "철저하게 분리"}, # 1이 예민

            # [사회성]
            "social_willingness": {0: "개인주의(혼자)", 1: "친목 도모(함께)"},
            "friend_invite": {0: "친구 초대 자제", 1: "친구 초대 환영"}, # 1이 Extrovert

            # [O/X 정보]
            "is_smoker": {True: "흡연자", False: "비흡연자"},
            "wants_smoker": {True: "흡연 룸메 OK", False: "비흡연 룸메 선호"},
            "is_drinker": {True: "음주 즐김", False: "비음주"},
            "wants_drinker": {True: "음주 룸메 OK", False: "비음주 룸메 선호"},
            "sensitive_heat": {True: "더위 많이 탐", False: "더위 잘 참음"},
            "sensitive_cold": {True: "추위 많이 탐", False: "추위 잘 참음"}
        }

        # 2. 항목 이름 한글 매핑 (결과 화면 표시용)
        self.col_name_map = {
            # 기본
            "sleep_habit": "취침시간", "wake_up": "기상시간", "activity_time": "주활동시간",
            "dorm_stay": "기숙사 체류", "out_return": "외출/복귀 연락",
            
            # 청결
            "clean_cycle": "청소주기", "clean_immediate": "정리습관", 
            "desk_status": "책상상태", "other_seat_tol": "타인영역 허용",
            
            # 예민
            "phone_noise": "통화소음", "light_sensitivity": "수면 등(Light)", 
            "key_mouse_noise": "타건/마우스 소음", "alarm_habit": "알람 습관",
            "space_privacy": "공용물품/공간",
            
            # 사회성
            "social_willingness": "사회성", "friend_invite": "친구 초대",
            
            # 호불호
            "is_smoker": "흡연여부", "wants_smoker": "흡연룸메 허용",
            "is_drinker": "음주여부", "wants_drinker": "음주룸메 허용",
            "sensitive_heat": "더위 민감도", "sensitive_cold": "추위 민감도"
        }

        self.feature_cols = list(self.weights.keys())

    def load_and_train(self):
        """서버 시작 시 데이터를 로드하고 모델을 학습합니다."""
        print("⏳ 데이터 로딩 및 모델 학습 시작...")
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        self.users_df = pd.DataFrame(data)

        # 1. 정규화 (Fit)
        features_norm = self.scaler.fit_transform(self.users_df[self.feature_cols])

        # 2. 가중치 적용
        weighted_data = features_norm.copy()
        for i, col in enumerate(self.feature_cols):
            weighted_data[:, i] *= self.weights[col]

        self.weighted_features_df = pd.DataFrame(weighted_data, columns=self.feature_cols, index=self.users_df.index)

        # 3. 클러스터링 학습
        self.users_df['cluster_id'] = self.kmeans.fit_predict(self.weighted_features_df)
        print("✅ 모델 학습 완료!")

    def preprocess_input(self, user_data: dict):
        """입력된 1명의 데이터를 기존 스케일러로 변환합니다."""
        input_df = pd.DataFrame([user_data])

        # 기존 scaler로 transform만 수행 (fit 아님)
        norm_data = self.scaler.transform(input_df[self.feature_cols])

        # 가중치 적용
        weighted_input = norm_data.copy()
        for i, col in enumerate(self.feature_cols):
            weighted_input[:, i] *= self.weights[col]

        return weighted_input # numpy array (1, n_features)

    def explain_match_detail(self, user_data: dict, partner_row: pd.Series):
        """상세 비교 로직"""
        match_items = []
        mismatch_items = []

        for col in self.feature_cols:
            val_me = user_data[col]
            val_partner = partner_row[col]
            col_name_kr = self.col_name_map.get(col, col)

            if val_me == val_partner:
                # 값이 같으면 일치 항목에 이름만 추가
                match_items.append(col_name_kr)
            else:
                # 값이 다르면 불일치 항목에 상세 내용 추가
                my_val_txt = self.text_map.get(col, {}).get(val_me, str(val_me))
                pt_val_txt = self.text_map.get(col, {}).get(val_partner, str(val_partner))

                mismatch_items.append({
                    "category": col_name_kr,
                    "my_value": my_val_txt,
                    "mate_value": pt_val_txt
                })

        return match_items, mismatch_items

    def recommend(self, user_data: dict, count=5, page=1):
        """API 요청 들어온 유저를 기반으로 추천"""
        target_student_id = user_data['student_id']
        target_gender = user_data['gender']

        # 1. 입력 데이터 벡터화 및 클러스터 예측
        target_vec = self.preprocess_input(user_data)
        target_cluster = self.kmeans.predict(target_vec)[0]

        # 2. 후보군 필터링 (같은 성별 AND 같은 클러스터)
        # 본인이 DB에 있을 경우 제외
        candidates = self.users_df[
            (self.users_df['student_id'] != target_student_id) &
            (self.users_df['gender'] == target_gender) &
            (self.users_df['cluster_id'] == target_cluster)
        ].copy()

        # 예외 처리: 후보 부족 시 같은 성별 전체로 확장
        if len(candidates) < count*page:
            candidates = self.users_df[
                (self.users_df['student_id'] != target_student_id) &
                (self.users_df['gender'] == target_gender)
            ].copy()

        # 3. 유사도 계산
        candidate_vecs = self.weighted_features_df.loc[candidates.index]
        sims = cosine_similarity(target_vec, candidate_vecs)[0]

        # 4. 결과 정리
        candidates['match_score'] = sims * 100
        # 점수순 정렬
        sorted_candidates = candidates.sort_values(by='match_score', ascending=False)
        
        # 페이징 계산 (예: page=2, count=5 이면 -> 5번 인덱스부터 10번 인덱스 전까지)
        start_idx = (page - 1) * count
        end_idx = start_idx + count
        
        # 해당 범위만큼 자르기 (범위를 벗어나면 알아서 빈 리스트가 됨)
        top_matches = sorted_candidates.iloc[start_idx:end_idx]

        results = []
        for _, row in top_matches.iterrows():
            m_items, mm_items = self.explain_match_detail(user_data, row)

            results.append({
                "student_id": row['student_id'],
                "major": row['major'],
                "match_rate": round(row['match_score'], 1),
                "is_smoker": bool(row['is_smoker']),
                "is_drinker": bool(row['is_drinker']),
                "sensitive_heat": bool(row.get('sensitive_heat', False)),
                "sensitive_cold": bool(row.get('sensitive_cold', False)),
                "match_items": m_items,      # 일치 항목 리스트
                "mismatch_items": mm_items   # 불일치 항목 상세 리스트
            })

        return results

app = FastAPI()

# Pydantic 모델 (입력 데이터 검증용)
class StudentInput(BaseModel):
    student_id: str
    age: int
    gender: str
    major: str
    is_smoker: bool
    wants_smoker: bool
    is_drinker: bool
    wants_drinker: bool
    sensitive_heat: bool
    sensitive_cold: bool
    sleep_habit: int
    wake_up: int
    activity_time: int
    clean_immediate: int
    desk_status: int
    clean_cycle: int
    out_return: int
    other_seat_tol: int
    phone_noise: int
    light_sensitivity: int
    key_mouse_noise: int
    space_privacy: int
    alarm_habit: int
    social_willingness: int
    friend_invite: int
    dorm_stay: int

# 전역 엔진 변수
engine = None

@app.on_event("startup")
def startup_event():
    global engine
    # 파일 경로가 맞는지 꼭 확인하세요!
    dummy_file_path = "dormitory_users.json"
    engine = DormMatchAI_Server(dummy_file_path)
    engine.load_and_train()

@app.post("/recommend")
def get_recommendation(user_input: StudentInput, count: int = 5, page: int = 1):
    if engine is None:
        raise HTTPException(status_code=500, detail="Model is not loaded")
    
    try:
        user_dict = user_input.dict()
        # count(몇 명씩)와 page(몇 번째 페이지)를 넘겨줍니다.
        recommendations = engine.recommend(user_dict, count=count, page=page)
        return recommendations
    except Exception as e:
        print(f"Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# 실행 테스트용 (이 파일 직접 실행 시)
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)